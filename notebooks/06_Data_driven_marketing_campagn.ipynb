{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e81e8ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RUNNING DATA-DRIVEN CAMPAIGN PIPELINE ===\n",
      "\n",
      "--- Building order_kpi ---\n",
      "order_kpi shape: (352358, 6)\n",
      "\n",
      "--- Defining churn label with cut-off ---\n",
      "Snapshot date: 2025-02-15 00:00:00\n",
      "Feature cut-off date (T0): 2024-11-17 00:00:00\n",
      "Churn distribution:\n",
      " churned\n",
      "1    79106\n",
      "0    69016\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Building behavioral features (pre-T0 only) ---\n",
      "Behavioral feature table shape: (101748, 8)\n",
      "\n",
      "--- Enriching with user & profile info ---\n",
      "features_df shape: (101748, 25)\n",
      "\n",
      "--- Building X_final and y ---\n",
      "Numerical features: ['freq_total', 'monetary_total', 'tenure_days', 'avg_order_value', 'orders_last_30d']\n",
      "Categorical features: ['source', 'language', 'sex', 'level', 'main_city']\n",
      "X_final shape: (101748, 400)\n",
      "Target distribution:\n",
      " churned\n",
      "1    79106\n",
      "0    22642\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Train / test split ---\n",
      "\n",
      "--- Training Random Forest churn model ---\n",
      "\n",
      "ROC-AUC: 0.7567\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.40      0.44      6793\n",
      "           1       0.84      0.88      0.86     23732\n",
      "\n",
      "    accuracy                           0.78     30525\n",
      "   macro avg       0.67      0.64      0.65     30525\n",
      "weighted avg       0.76      0.78      0.77     30525\n",
      "\n",
      "\n",
      "--- Retraining model on full data for scoring ---\n",
      "\n",
      "--- Scoring all customers and selecting campaign targets ---\n",
      "Monetary cutoff (top 50%): 125.00\n",
      "Number of campaign targets: 34861\n",
      "\n",
      " Campaign targets saved to: campaign_targets.csv\n",
      "=== PIPELINE COMPLETED ===\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import dill\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Adjust sys.path to include the project root for src module\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import utils (ensure src/unimib_snowit_project/utils.py exists)\n",
    "try:\n",
    "    import src.unimib_snowit_project.utils as u\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Error: src module not found. Ensure src/unimib_snowit_project/utils.py exists in the project root.\")\n",
    "    raise\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "\n",
    "# CONFIGURATION\n",
    "\n",
    "DATA_PKL_DIR = project_root / \"data_loaded\"  # Correct path to parent directory\n",
    "\n",
    "USERS_PKL_FILENAME = \"users.pkl\"\n",
    "PROFILES_PKL_FILENAME = \"profiles.pkl\"\n",
    "CARDS_PKL_FILENAME = \"cards.pkl\"\n",
    "ORDERS_PKL_FILENAME = \"orders.pkl\"\n",
    "ORDER_DETAILS_PKL_FILENAME = \"order_details.pkl\"\n",
    "\n",
    "CHURN_PERIOD_DAYS = 90           # inactivity window that defines churn\n",
    "CHURN_PROB_THRESHOLD = 0.65      # min churn probability to target\n",
    "VALUE_PERCENTILE = 0.5           # top X% by monetary value (0.5 = top 50%)\n",
    "OUTPUT_TARGETS_CSV = \"campaign_targets.csv\"\n",
    "\n",
    "# HELPERS\n",
    "\n",
    "def load_pkl(path: Path):\n",
    "    with path.open(\"rb\") as fh:\n",
    "        return dill.load(fh)\n",
    "\n",
    "# ... (rest of the helpers and main function remain unchanged)\n",
    "\n",
    "def build_order_kpi(orders_df, order_details_df):\n",
    "    \"\"\"Builds an order-level KPI table from raw orders and order_details.\"\"\"\n",
    "    print(\"\\n--- Building order_kpi ---\")\n",
    "    \n",
    "    # Keep only fulfilled items\n",
    "    order_details_clean = order_details_df[order_details_df[\"item.status\"] == \"fulfilled\"].copy()\n",
    "    \n",
    "    #  Parse dates\n",
    "    orders_df[\"createdAt\"] = pd.to_datetime(orders_df[\"createdAt\"])\n",
    "    \n",
    "    # Join items â†’ orders\n",
    "    od_orders = order_details_clean.merge(\n",
    "        orders_df[[\"order.uid\", \"user.uid\", \"createdAt\", \"source\", \"tenant\"]],\n",
    "        on=\"order.uid\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    #  Item revenue\n",
    "    od_orders[\"item_revenue\"] = od_orders[\"item.amount\"].astype(float)\n",
    "    \n",
    "    #  Aggregate to order level\n",
    "    order_kpi = (\n",
    "        od_orders\n",
    "        .groupby(\"order.uid\", as_index=False)\n",
    "        .agg(\n",
    "            user_uid=(\"user.uid\", \"first\"),\n",
    "            order_date=(\"createdAt\", \"first\"),\n",
    "            source=(\"source\", \"first\"),\n",
    "            tenant=(\"tenant\", \"first\"),\n",
    "            order_revenue=(\"item_revenue\", \"sum\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\"order_kpi shape:\", order_kpi.shape)\n",
    "    return order_kpi\n",
    "\n",
    "\n",
    "def define_churn_label(order_kpi, churn_period_days=90):\n",
    "    \"\"\"\n",
    "    Define churn label using a proper cut-off:\n",
    "    - T0 = snapshot_date - churn_period_days\n",
    "    - churned = no order after T0\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Defining churn label with cut-off ---\")\n",
    "    \n",
    "    snapshot_date = order_kpi[\"order_date\"].max()\n",
    "    cutoff_date = snapshot_date - pd.Timedelta(days=churn_period_days)\n",
    "    \n",
    "    print(\"Snapshot date:\", snapshot_date)\n",
    "    print(\"Feature cut-off date (T0):\", cutoff_date)\n",
    "    \n",
    "    # Orders BEFORE T0 (for feature engineering)\n",
    "    orders_before = order_kpi[order_kpi[\"order_date\"] < cutoff_date].copy()\n",
    "    \n",
    "    # Last order over entire window (for label)\n",
    "    last_order_overall = (\n",
    "        order_kpi\n",
    "        .groupby(\"user_uid\")[\"order_date\"]\n",
    "        .max()\n",
    "        .rename(\"last_order_date\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # churned = last_order_date < T0\n",
    "    last_order_overall[\"churned\"] = (last_order_overall[\"last_order_date\"] < cutoff_date).astype(int)\n",
    "    \n",
    "    print(\"Churn distribution:\\n\", last_order_overall[\"churned\"].value_counts())\n",
    "    return orders_before, cutoff_date, last_order_overall\n",
    "\n",
    "\n",
    "def build_behavior_features(orders_before, cutoff_date):\n",
    "    \"\"\"Builds behavioral features using ONLY data before cutoff_date (no leakage).\"\"\"\n",
    "    print(\"\\n--- Building behavioral features (pre-T0 only) ---\")\n",
    "    \n",
    "    agg_before = (\n",
    "        orders_before\n",
    "        .groupby(\"user_uid\")\n",
    "        .agg(\n",
    "            first_order_date=(\"order_date\", \"min\"),\n",
    "            last_order_before_T0=(\"order_date\", \"max\"),\n",
    "            freq_total=(\"order.uid\", \"nunique\"),\n",
    "            monetary_total=(\"order_revenue\", \"sum\")\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Tenure\n",
    "    agg_before[\"tenure_days\"] = (cutoff_date - agg_before[\"first_order_date\"]).dt.days\n",
    "    \n",
    "    # Avg order value\n",
    "    agg_before[\"avg_order_value\"] = agg_before[\"monetary_total\"] / agg_before[\"freq_total\"]\n",
    "    \n",
    "    # Orders in last 30 days before T0\n",
    "    window_start_30 = cutoff_date - pd.Timedelta(days=30)\n",
    "    orders_last_30 = (\n",
    "        orders_before[\n",
    "            (orders_before[\"order_date\"] >= window_start_30) &\n",
    "            (orders_before[\"order_date\"] < cutoff_date)\n",
    "        ]\n",
    "        .groupby(\"user_uid\")[\"order.uid\"]\n",
    "        .nunique()\n",
    "        .rename(\"orders_last_30d\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    features_behavior = agg_before.merge(orders_last_30, on=\"user_uid\", how=\"left\")\n",
    "    features_behavior[\"orders_last_30d\"] = features_behavior[\"orders_last_30d\"].fillna(0)\n",
    "    \n",
    "    print(\"Behavioral feature table shape:\", features_behavior.shape)\n",
    "    return features_behavior\n",
    "\n",
    "\n",
    "def enrich_with_user_profile(features_behavior, users_df, profiles_df, last_order_overall):\n",
    "    \"\"\"Merge behavior + churn labels + users + profile info.\"\"\"\n",
    "    print(\"\\n--- Enriching with user & profile info ---\")\n",
    "    \n",
    "    users_ren = users_df.rename(columns={\"user.uid\": \"user_uid\"})\n",
    "    profiles_ren = profiles_df.rename(columns={\"user.uid\": \"user_uid\"})\n",
    "    \n",
    "    user_profile = (\n",
    "        profiles_ren\n",
    "        .groupby(\"user_uid\", as_index=False)\n",
    "        .agg(\n",
    "            main_city=(\"city\", \"first\"),\n",
    "            sex=(\"sex\", \"first\"),\n",
    "            level=(\"level\", \"first\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    features_df = (\n",
    "        features_behavior\n",
    "        .merge(last_order_overall[[\"user_uid\", \"churned\"]], on=\"user_uid\", how=\"inner\")\n",
    "        .merge(users_ren, on=\"user_uid\", how=\"left\")\n",
    "        .merge(user_profile, on=\"user_uid\", how=\"left\")\n",
    "    )\n",
    "    \n",
    "    print(\"features_df shape:\", features_df.shape)\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def build_design_matrix(features_df):\n",
    "    \"\"\"Build X_final and y with manual preprocessing (no sklearn ColumnTransformer).\"\"\"\n",
    "    print(\"\\n--- Building X_final and y ---\")\n",
    "    \n",
    "    # Candidate features\n",
    "    candidate_num = [\"freq_total\", \"monetary_total\", \"tenure_days\", \"avg_order_value\", \"orders_last_30d\"]\n",
    "    candidate_cat = [\"source\", \"language\", \"sex\", \"level\", \"main_city\"]\n",
    "    \n",
    "    num_features = [c for c in candidate_num if c in features_df.columns]\n",
    "    cat_features = [c for c in candidate_cat if c in features_df.columns]\n",
    "    \n",
    "    print(\"Numerical features:\", num_features)\n",
    "    print(\"Categorical features:\", cat_features)\n",
    "    \n",
    "    # Clean NA types\n",
    "    features_df_clean = features_df.replace({pd.NA: np.nan})\n",
    "    \n",
    "    # Numeric part\n",
    "    X_num = features_df_clean[num_features].astype(\"float64\")\n",
    "    X_num_imputed = X_num.fillna(X_num.median())\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_num_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_num_imputed),\n",
    "        columns=num_features,\n",
    "        index=features_df_clean.index\n",
    "    )\n",
    "    \n",
    "    # Categorical part\n",
    "    X_cat = features_df_clean[cat_features].astype(\"string\")\n",
    "    X_cat_imputed = X_cat.fillna(\"Missing\")\n",
    "    X_cat_dummies = pd.get_dummies(X_cat_imputed, drop_first=False)\n",
    "    \n",
    "    # Final matrix\n",
    "    X_final = pd.concat([X_num_scaled, X_cat_dummies], axis=1)\n",
    "    \n",
    "    y = features_df_clean[\"churned\"].astype(int)\n",
    "    \n",
    "    print(\"X_final shape:\", X_final.shape)\n",
    "    print(\"Target distribution:\\n\", y.value_counts())\n",
    "    \n",
    "    return X_final, y, num_features, cat_features\n",
    "\n",
    "\n",
    "def train_churn_model(X_final, y):\n",
    "    \"\"\"Train a RandomForest churn model and print evaluation on a hold-out set.\"\"\"\n",
    "    print(\"\\n--- Train / test split ---\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_final,\n",
    "        y,\n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- Training Random Forest churn model ---\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"\\nROC-AUC: {auc:.4f}\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Retrain on full data for campaign scoring\n",
    "    print(\"\\n--- Retraining model on full data for scoring ---\")\n",
    "    model.fit(X_final, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def select_campaign_targets(features_df, model, X_final, prob_threshold=0.65, value_percentile=0.5):\n",
    "    \"\"\"\n",
    "    Score all customers with churn probability, then select campaign targets:\n",
    "    - churn_proba >= prob_threshold\n",
    "    - monetary_total in top value_percentile\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Scoring all customers and selecting campaign targets ---\")\n",
    "    \n",
    "    churn_proba = model.predict_proba(X_final)[:, 1]\n",
    "    \n",
    "    scored = features_df.copy()\n",
    "    scored[\"churn_proba\"] = churn_proba\n",
    "    \n",
    "    # High value cutoff based on monetary_total\n",
    "    monetary_cutoff = scored[\"monetary_total\"].quantile(value_percentile)\n",
    "    print(f\"Monetary cutoff (top {int((1 - value_percentile)*100)}%): {monetary_cutoff:.2f}\")\n",
    "    \n",
    "    targets = scored[\n",
    "        (scored[\"churn_proba\"] >= prob_threshold) &\n",
    "        (scored[\"monetary_total\"] >= monetary_cutoff)\n",
    "    ].copy()\n",
    "    \n",
    "    print(\"Number of campaign targets:\", len(targets))\n",
    "    \n",
    "    # Select columns useful for marketing export\n",
    "    export_cols = [\n",
    "        \"user_uid\", \"churn_proba\", \"monetary_total\", \"freq_total\",\n",
    "        \"tenure_days\", \"orders_last_30d\", \"main_city\", \"sex\", \"level\", \"source\"\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in targets.columns]\n",
    "    \n",
    "    return targets[export_cols]\n",
    "\n",
    "\n",
    "# MAIN EXECUTION\n",
    "\n",
    "def main():\n",
    "    print(\"=== RUNNING DATA-DRIVEN CAMPAIGN PIPELINE ===\")\n",
    "    \n",
    "    #  Load PKL data\n",
    "    users_df = load_pkl(DATA_PKL_DIR / USERS_PKL_FILENAME)\n",
    "    profiles_df = load_pkl(DATA_PKL_DIR / PROFILES_PKL_FILENAME)\n",
    "    cards_df = load_pkl(DATA_PKL_DIR / CARDS_PKL_FILENAME)\n",
    "    orders_df = load_pkl(DATA_PKL_DIR / ORDERS_PKL_FILENAME)\n",
    "    order_details_df = load_pkl(DATA_PKL_DIR / ORDER_DETAILS_PKL_FILENAME)\n",
    "    \n",
    "    #  Build order_kpi\n",
    "    order_kpi = build_order_kpi(orders_df, order_details_df)\n",
    "    \n",
    "    # Define churn label\n",
    "    orders_before, cutoff_date, last_order_overall = define_churn_label(\n",
    "        order_kpi, churn_period_days=CHURN_PERIOD_DAYS\n",
    "    )\n",
    "    \n",
    "    #  Build behavioral features\n",
    "    features_behavior = build_behavior_features(orders_before, cutoff_date)\n",
    "    \n",
    "    #  Enrich with user + profile & churn label\n",
    "    features_df = enrich_with_user_profile(\n",
    "        features_behavior, users_df, profiles_df, last_order_overall\n",
    "    )\n",
    "    \n",
    "    #  Build design matrix\n",
    "    X_final, y, num_features, cat_features = build_design_matrix(features_df)\n",
    "    \n",
    "    #  Train churn model\n",
    "    model = train_churn_model(X_final, y)\n",
    "    \n",
    "    #  Select campaign targets\n",
    "    targets_df = select_campaign_targets(\n",
    "        features_df,\n",
    "        model,\n",
    "        X_final,\n",
    "        prob_threshold=CHURN_PROB_THRESHOLD,\n",
    "        value_percentile=VALUE_PERCENTILE\n",
    "    )\n",
    "    \n",
    "    #  Save targets for marketing\n",
    "    targets_df.to_csv(OUTPUT_TARGETS_CSV, index=False)\n",
    "    print(f\"\\n Campaign targets saved to: {OUTPUT_TARGETS_CSV}\")\n",
    "    print(\"=== PIPELINE COMPLETED ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5d067-c290-472f-bcc4-43702b9ab975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
